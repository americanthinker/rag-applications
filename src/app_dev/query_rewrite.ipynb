{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "#load from local .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "env = load_dotenv('.env', override=True)\n",
    "\n",
    "#standard python\n",
    "from typing import List, Dict, Tuple\n",
    "import os\n",
    "\n",
    "# external libraries\n",
    "from tqdm import tqdm\n",
    "from rich import print  # nice library that provides improved printing output (overrides default print function)\n",
    "from openai import OpenAI\n",
    "\n",
    "# external files\n",
    "from src.reranker import ReRanker\n",
    "from src.database.weaviate_interface_v4 import WeaviateWCS\n",
    "from src.app_dev.app_functions import validate_token_threshold\n",
    "from src.app_dev.query import (\n",
    "    parse_context_results,\n",
    "    CompletedQuery,\n",
    "    CompletedQueryQueue\n",
    ")\n",
    "from tiktoken import Encoding, get_encoding\n",
    "from src.llm.prompt_templates import (\n",
    "    question_answering_prompt_series,\n",
    "    generate_prompt_series,\n",
    "    huberman_system_message\n",
    ")\n",
    "from src.llm.llm_utils import load_azure_openai\n",
    "from src.llm.llm_interface import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read env vars from local .env file\n",
    "api_key = os.environ['WEAVIATE_API_KEY']\n",
    "url = os.environ['WEAVIATE_ENDPOINT']\n",
    "\n",
    "#instantiate client\n",
    "client = WeaviateWCS(url, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display available collection names on cluster\n",
    "client.show_all_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set collection name to run queries on\n",
    "collection_name = 'Huberman_minilm_256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial query\n",
    "query = 'Does he reference exactly what literature?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.45\n",
    "hyb_response = client.hybrid_search(query, collection_name, alpha=alpha, limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = ReRanker()\n",
    "reranked_results = reranker.rerank(\n",
    "            hyb_response, query, apply_sigmoid=False, top_k=5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = get_encoding(\"cl100k_base\")\n",
    "\n",
    "valid_results = validate_token_threshold(\n",
    "            ranked_results=reranked_results,\n",
    "            system_message=question_answering_prompt_series,\n",
    "            query=query,\n",
    "            tokenizer=encoding,\n",
    "            token_threshold=4000,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting this here so have for the initial LLM call that we currently use.\n",
    "context_series = generate_prompt_series(query, valid_results, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = load_azure_openai('gpt-35-turbo')\n",
    "\n",
    "llm = LLM('gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = llm.chat_completion(huberman_system_message,\n",
    "    user_message=context_series,\n",
    "    temperature=0.5,\n",
    "    max_tokens=1000)\n",
    "    \n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, I just want title, guest, and content.\n",
    "context_results = parse_context_results(valid_results)\n",
    "# Create a CompletedQuery object\n",
    "completed_query = CompletedQuery(query, context_results, llm_response)\n",
    "\n",
    "# Add the CompletedQuery object to the CompletedQueryQueue\n",
    "# This is how we'll store the last 5 queries.\n",
    "completed_query_queue = CompletedQueryQueue()\n",
    "completed_query_queue.add_query(completed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to be my vague follow-up query.\n",
    "new_user_query = \"Who else discusses that topic?\"\n",
    "\n",
    "# We need to convert the data structure holding the last 5 queries to a string\n",
    "# so we can send it to the llm.\n",
    "completed_query_string_list = completed_query_queue.to_string()\n",
    "\n",
    "# Combine the new vague query with the previous queries and their contexts, and \n",
    "# answers to create a new llm prompt.\n",
    "query_rewrite_prompt = create_llm_prompt(new_user_query, completed_query_string_list)\n",
    "print(query_rewrite_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what we'd get currently.\n",
    "llm_response = llm.chat_completion(huberman_system_message,\n",
    "    user_message=new_user_query,\n",
    "    temperature=0.5,\n",
    "    max_tokens=1000)\n",
    "\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was using this system message but haven't tried using the old one. \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# This is what the new prompts get with the vague question.\n",
    "llm_response = llm.chat_completion(huberman_system_message,\n",
    "    user_message=query_rewrite_prompt,\n",
    "    temperature=0.5,\n",
    "    max_tokens=1000)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class ContextResult:\n",
    "    title: str\n",
    "    guest: str\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class CompletedQuery:\n",
    "    user_query: str\n",
    "    context_results_list: List[ContextResult]\n",
    "    llm_answer: str\n",
    "    llm_revised_query: str = None\n",
    "\n",
    "class CompletedQueryQueue:\n",
    "    def __init__(self):\n",
    "        self.completed_query_list: deque[CompletedQuery] = deque(maxlen=5)\n",
    "\n",
    "def format_completed_query_queue(queue: CompletedQueryQueue) -> str:\n",
    "    formatted_queries = []\n",
    "    for i, query in enumerate(queue.completed_query_list, 1):\n",
    "        formatted_results = []\n",
    "        for j, result in enumerate(query.context_results_list, 1):\n",
    "            formatted_results.append(\n",
    "                f\"Context Result {j}:\\n\"\n",
    "                f\"Title: {result.title}\\n\"\n",
    "                f\"Guest: {result.guest}\\n\"\n",
    "                f\"Content: {result.content}\\n\"\n",
    "            )\n",
    "        formatted_query = (\n",
    "            f\"Query {i}:\\n\"\n",
    "            f\"User Query: {query.user_query}\\n\"\n",
    "            f\"LLM Answer: {query.llm_answer}\\n\"\n",
    "            f\"LLM Revised Query: {query.llm_revised_query if query.llm_revised_query else 'None'}\\n\"\n",
    "            f\"{''.join(formatted_results)}\"\n",
    "        )\n",
    "        formatted_queries.append(formatted_query)\n",
    "    return \"\\n\\n\".join(formatted_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "cq1 = CompletedQuery(\n",
    "    user_query=\"Who was the first president of the United States?\",\n",
    "    context_results_list=[\n",
    "        ContextResult(title=\"Biography of George Washington\", guest=\"Historian A\", content=\"George Washington was the first president...\"),\n",
    "        ContextResult(title=\"Presidency Overview\", guest=\"Expert B\", content=\"He served from 1789 to 1797...\")\n",
    "    ],\n",
    "    llm_answer=\"George Washington was the first president of the United States.\",\n",
    ")\n",
    "\n",
    "cq2 = CompletedQuery(\n",
    "    user_query=\"What were his accomplishments?\",\n",
    "    context_results_list=[\n",
    "        ContextResult(title=\"George Washington's Achievements\", guest=\"Historian A\", content=\"He led the Continental Army...\"),\n",
    "        ContextResult(title=\"Presidential Achievements\", guest=\"Expert B\", content=\"He established many protocols...\"),\n",
    "    ],\n",
    "    llm_answer=\"George Washington had many accomplishments including leading the Continental Army to victory...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = CompletedQueryQueue()\n",
    "queue.completed_query_list.extend([cq1, cq2])\n",
    "\n",
    "formatted_string = format_completed_query_queue(queue)\n",
    "print(formatted_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
